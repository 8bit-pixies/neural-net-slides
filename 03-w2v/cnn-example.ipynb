{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from faker import Factory\n",
    "# use example here:\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "from model_data import create_data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, label = create_data_sample()\n",
    "max_len = max([len(x.split()) for x in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split in sklearn\n",
    "x_train_text, x_test_text, y_train,  y_test = train_test_split(data, label, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_to_seq = text.Tokenizer()\n",
    "sent_to_seq.fit_on_texts(x_train_text)\n",
    "x_train = sequence.pad_sequences(sent_to_seq.texts_to_sequences(x_train_text), maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(sent_to_seq.texts_to_sequences(x_test_text), maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cnn settings\n",
    "max_features = 5000\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 600 samples\n",
      "Epoch 1/2\n",
      "1400/1400 [==============================] - 2s - loss: 0.3038 - acc: 0.9121 - val_loss: 6.4879e-04 - val_acc: 1.0000\n",
      "Epoch 2/2\n",
      "1400/1400 [==============================] - 1s - loss: 1.5242e-04 - acc: 1.0000 - val_loss: 6.3640e-05 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cd2ef41ef0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=max_len))\n",
    "# model.add(Dropout(0.2)) # this layer is a regularization layer\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims, activation='relu', name='wordembedding')) # this is the word embedding if you wish to keep it, we can always extract it later\n",
    "# model.add(Dropout(0.2)) # regularization layer\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(2)) # as we have a classification problem the final layer is 2, if we have multi-class (say 3 classes then this would be `Dense(3)`.\n",
    "model.add(Activation('softmax')) # if we use softmax, then we have softmax AKA multinomial regression\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=2,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 28, 50)            250000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 26, 250)           37750     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "wordembedding (Dense)        (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 502       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 351,002\n",
      "Trainable params: 351,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# show performance...this is rather crude but works in this instance\n",
    "# normally we would want to set a threshold\n",
    "yh_train = np.argmax(model.predict(x_train), axis=1)\n",
    "yh_test = np.argmax(model.predict(x_test), axis=1)\n",
    "# Accuracy should be very very high\n",
    "print(\"Train accuracy: {}\".format(accuracy_score(y_train, yh_train)))\n",
    "print(\"Test accuracy: {}\".format(accuracy_score(y_test, yh_test)))\n",
    " \n",
    "# get the output as a vector:\n",
    "word_embedding = Model(inputs=model.input,\n",
    "                       outputs=model.get_layer(name='wordembedding').output)\n",
    "# this will output your word embedding of size 250.\n",
    "word_train = word_embedding.predict(x_train)\n",
    "word_test = word_embedding.predict(x_test)\n",
    "# you can verify using `word_train.shape` or `word_test.shape`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl-uts]",
   "language": "python",
   "name": "conda-env-dl-uts-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
