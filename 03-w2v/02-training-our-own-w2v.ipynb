{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grade 1 reading example from: http://www.pearsonlongman.com/ae/marketing/sfesl/tests/grade1.html\n",
    "\n",
    "sample_text1 = \"\"\"Saturday is our day to clean, but Grandpa turns work into fun.\n",
    "\n",
    "We like to sing when we dust. We like to dance when we mop. We clean the car together. We do the wash together. We both wear our caps.\n",
    "\n",
    "When we are done, we hurry to the store. We shop for food. Grandpa lets me pick the fruits that I like best. We carry the food home. Grandpa tells stories while we walk.\n",
    "\n",
    "At night, our work is done. Friends come over. We cook dinner. We sing and dance.\n",
    "\n",
    "Saturday is our day to clean, but Grandpa turns work into fun!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sample_text2 = \"\"\"How does a butterfly grow? It starts out as a tiny egg. It becomes a caterpillar. It eats lots of leaves. It grows and grows. Then it goes inside a cocoon. At last, it comes out. It’s a butterfly!\n",
    "\n",
    "How does a frog grow? It starts out as a tiny egg in the water. The egg grows into a tadpole. It keeps changing. It eats tiny plants. It grows and grows. At last, it hops out of the pond. It’s a frog!\n",
    "\n",
    "How does a flower grow? It starts out as a seed. Sun and rain help the seed grow. Roots grow into the ground. The plant grows and grows. At last, a bud opens. It’s a flower!\n",
    "\n",
    "Now you know how they grow!\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build training data...using skip-gram\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def n_gram_build(text, full_window):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    *  text: sample text which skipgram will be applied\n",
    "    *  window_size: skip gram parameter for window size.\n",
    "    \n",
    "    technically window_size is the full window size, but i've done it this way for easier programming...\n",
    "    \"\"\"\n",
    "    # some check to see full window size is odd\n",
    "    window_size = (full_window-1)/2\n",
    "    # some basic text cleaning\n",
    "    text = re.sub(r\"[^0-9a-z]\", \" \", text.lower())\n",
    "    text = re.sub(r\"\\s+\", \" \", text.strip())\n",
    "    \n",
    "    full_vocab = np.array(list(set(text.split())))\n",
    "    \n",
    "    padded = [\" \"] * (full_window-1)\n",
    "    #tokens = padded+text.split()+padded\n",
    "    tokens = text.split()\n",
    "    \n",
    "    zip_list = []\n",
    "    for idx in range(full_window):\n",
    "        if idx == full_window-1:\n",
    "            zip_list.append(tokens[idx:])\n",
    "        else:\n",
    "            zip_list.append(tokens[idx:-(full_window-1-idx)])\n",
    "            \n",
    "    ngram_tokens = list(zip(*zip_list))\n",
    "        \n",
    "    # now create ngram\n",
    "    return ngram_tokens, full_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('saturday', 'is', 'our'), ('is', 'our', 'day'), ('our', 'day', 'to'), ('day', 'to', 'clean'), ('to', 'clean', 'but')]\n"
     ]
    }
   ],
   "source": [
    "print(n_gram_build(sample_text1, 3)[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_gram_build_split(ngram_tokens, cv_mod):\n",
    "    \"\"\"\n",
    "    ngram tokens as produced in the ngram function\n",
    "    \n",
    "    we simply take the middle element to be the label, and the rest to be\n",
    "    the context.\n",
    "    \"\"\"\n",
    "    mid_point = int(((len(ngram_tokens[0])+1)/2)-1) # zero indexing\n",
    "    def create_label_point(context, mid_point):\n",
    "        mid_point = int(mid_point)\n",
    "        context = list(context)\n",
    "        target = context.pop(mid_point)\n",
    "        return (np.array(cv_1.transform(context).todense()), np.array(cv_1.transform([target]).todense()).flatten())\n",
    "    skip_gram_list = [create_label_point(tt, mid_point) for tt in ngram_tokens]\n",
    "    return skip_gram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# before we train in keras, we need to know the input and output size, \n",
    "# in this case the input AND output will be the size of our vocabulary\n",
    "# we will need to transform it into binary text blobs.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "ngrams_text1, vocab1 = n_gram_build(sample_text1,3)\n",
    "vocab_size1 = len(vocab1)\n",
    "cv_1 = CountVectorizer(vocabulary=vocab1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([], shape=(0, 55), dtype=int64), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)), (array([], shape=(0, 55), dtype=int64), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64))]\n"
     ]
    }
   ],
   "source": [
    "print(n_gram_build_split(n_gram_build(sample_text1,1)[0], cv_1)[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# window size = \n",
    "window_size = 3\n",
    "train_set1 = n_gram_build_split(n_gram_build(sample_text1, window_size)[0], cv_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context1 = np.array([x[0] for x in train_set1])\n",
    "target1  = np.array([x[1] for x in train_set1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now plug into keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape, Merge\n",
    "\n",
    "# create CBOW model\n",
    "embedding_size = vocab_size1\n",
    "embedding_size = 10\n",
    "\n",
    "model = Sequential()\n",
    "# our input and output is the same size\n",
    "model.add(Dense(vocab_size1, input_shape=(window_size-1, vocab_size1,), name='input_layer')) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(embedding_size, name='embedding_layer'))\n",
    "model.add(Dense(vocab_size1, name='output_layer'))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 21.8 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# CBOW\n",
    "model.fit(context1, target1, verbose=0, batch_size=5, nb_epoch=1000) # feel free to change verbose to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_layer (Dense)              (None, 2, 55)         3080        dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 2, 55)         0           input_layer[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 110)           0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "embedding_layer (Dense)          (None, 10)            1110        flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "output_layer (Dense)             (None, 55)            605         embedding_layer[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 55)            0           output_layer[0][0]               \n",
      "====================================================================================================\n",
      "Total params: 4,795\n",
      "Trainable params: 4,795\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.92\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(context1, target1, verbose=0)\n",
    "print(\"Accuracy is {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.10413742, -2.65807986,  2.89546251, -0.52381462,  0.46597743,\n",
       "        3.92889667, -2.4014895 , -6.09131289, -1.42299211, -2.1180861 ], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = Model(input=model.input, \n",
    "                  output=model.get_layer('embedding_layer').output)\n",
    "embedding_vector = embedding.predict(np.expand_dims(context1[0,:,:], axis=0))\n",
    "embedding_vector.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# skip gram\n",
    "\n",
    "model = Sequential()\n",
    "# our input and output is the same size\n",
    "model.add(Dense(vocab_size1, input_shape=(vocab_size1,), name='input_layer')) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(embedding_size, name='embedding_layer'))\n",
    "\"\"\"\n",
    "model.add(Merge([Sequential([Dense((window_size-1)*vocab_size1, input_shape=(embedding_size, ))]), \n",
    "                 Sequential([Reshape((window_size-1, vocab_size1), input_shape=((window_size-1)*vocab_size1,))])], \n",
    "                name='output_layer', mode='concat'))\n",
    "\"\"\"\n",
    "model.add(Dense((window_size-1)*vocab_size1))\n",
    "model.add(Reshape((window_size-1, vocab_size1), name='output_layer'))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 25.3 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# skip-gram\n",
    "model.fit(target1, context1, verbose=0, batch_size=5, nb_epoch=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.68\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(target1, context1, verbose=0)\n",
    "print(\"Accuracy is {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.92829096, -5.07062626, -9.21794701, -1.31392336,  3.94425321,\n",
       "        -0.80875969,  3.23186731, -3.33245945,  0.34641635,  2.22713947]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = Model(input=model.input, \n",
    "                  output=model.get_layer('embedding_layer').output)\n",
    "embedding_vector = embedding.predict(np.expand_dims(target1[0], axis=0))\n",
    "embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_layer (Dense)              (None, 55)            3080        dense_input_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 55)            0           input_layer[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "embedding_layer (Dense)          (None, 10)            560         activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 110)           1210        embedding_layer[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "output_layer (Reshape)           (None, 2, 55)         0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 2, 55)         0           output_layer[0][0]               \n",
      "====================================================================================================\n",
      "Total params: 4,850\n",
      "Trainable params: 4,850\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Considerations\n",
    "--------------------\n",
    "\n",
    "*  In the current context of word2vec we have only provided _positive_ examples. Positive examples seek to promote vectors so that they are more similar. Negative sampling can be used to perhaps overcome the bias within the model which favours more common words. In this case we will randomly generate dissimilar training data in order to improve the word2vec model. This is what the negative sampling parameter refers to. Skip-gram implementations seems to benefit greatly from negative sampling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**When to use Skipgram? When to use CBOW?**\n",
    "\n",
    "> Skip-gram: works well with small amount of the training data, represents well even rare words or phrases.\n",
    "CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words\n",
    "This can get even a bit more complicated if you consider that there are two different ways how to train the models: the normalized hierarchical softmax, and the un-normalized negative sampling. Both work quite differently. - Mikolov, author of Word2Vec Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
